{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from geopy.geocoders import Nominatim\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_pipelines import *\n",
    "from kacper_pipelines import *\n",
    "from pipelines_miki import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = './weatherAUS.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>...</th>\n",
       "      <th>Humidity9am</th>\n",
       "      <th>Humidity3pm</th>\n",
       "      <th>Pressure9am</th>\n",
       "      <th>Pressure3pm</th>\n",
       "      <th>Cloud9am</th>\n",
       "      <th>Cloud3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>RainTomorrow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-12-01</td>\n",
       "      <td>Albury</td>\n",
       "      <td>13.4</td>\n",
       "      <td>22.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>44.0</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>71.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1007.7</td>\n",
       "      <td>1007.1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.9</td>\n",
       "      <td>21.8</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-12-02</td>\n",
       "      <td>Albury</td>\n",
       "      <td>7.4</td>\n",
       "      <td>25.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WNW</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1010.6</td>\n",
       "      <td>1007.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.2</td>\n",
       "      <td>24.3</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-12-03</td>\n",
       "      <td>Albury</td>\n",
       "      <td>12.9</td>\n",
       "      <td>25.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WSW</td>\n",
       "      <td>46.0</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1007.6</td>\n",
       "      <td>1008.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-12-04</td>\n",
       "      <td>Albury</td>\n",
       "      <td>9.2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NE</td>\n",
       "      <td>24.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1017.6</td>\n",
       "      <td>1012.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.1</td>\n",
       "      <td>26.5</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-12-05</td>\n",
       "      <td>Albury</td>\n",
       "      <td>17.5</td>\n",
       "      <td>32.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>41.0</td>\n",
       "      <td>ENE</td>\n",
       "      <td>...</td>\n",
       "      <td>82.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1010.8</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>29.7</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date Location  MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine  \\\n",
       "0  2008-12-01   Albury     13.4     22.9       0.6          NaN       NaN   \n",
       "1  2008-12-02   Albury      7.4     25.1       0.0          NaN       NaN   \n",
       "2  2008-12-03   Albury     12.9     25.7       0.0          NaN       NaN   \n",
       "3  2008-12-04   Albury      9.2     28.0       0.0          NaN       NaN   \n",
       "4  2008-12-05   Albury     17.5     32.3       1.0          NaN       NaN   \n",
       "\n",
       "  WindGustDir  WindGustSpeed WindDir9am  ... Humidity9am  Humidity3pm  \\\n",
       "0           W           44.0          W  ...        71.0         22.0   \n",
       "1         WNW           44.0        NNW  ...        44.0         25.0   \n",
       "2         WSW           46.0          W  ...        38.0         30.0   \n",
       "3          NE           24.0         SE  ...        45.0         16.0   \n",
       "4           W           41.0        ENE  ...        82.0         33.0   \n",
       "\n",
       "   Pressure9am  Pressure3pm  Cloud9am  Cloud3pm  Temp9am  Temp3pm  RainToday  \\\n",
       "0       1007.7       1007.1       8.0       NaN     16.9     21.8         No   \n",
       "1       1010.6       1007.8       NaN       NaN     17.2     24.3         No   \n",
       "2       1007.6       1008.7       NaN       2.0     21.0     23.2         No   \n",
       "3       1017.6       1012.8       NaN       NaN     18.1     26.5         No   \n",
       "4       1010.8       1006.0       7.0       8.0     17.8     29.7         No   \n",
       "\n",
       "   RainTomorrow  \n",
       "0            No  \n",
       "1            No  \n",
       "2            No  \n",
       "3            No  \n",
       "4            No  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "australia_rain = pd.read_csv(FILE_PATH)\n",
    "australia_rain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WholeRainClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, clf_class = RandomForestClassifier(n_jobs=-1),\n",
    "                 columns_na_threshold = 12,\n",
    "                 temp_daily_difference = True,\n",
    "                 wind_daily_difference = True,\n",
    "                 pressure_daily_difference = True,\n",
    "                 humidity_daily_difference = True,\n",
    "                 latitude_longnitude = True,\n",
    "                 imputation = \"mean\",\n",
    "                 wind_transformation = \"wind_to_degrees\",\n",
    "                 date_features = True\n",
    "                ):\n",
    "        self.clf_class = clf_class\n",
    "        self.columns_na_threshold = 12\n",
    "        self.temp_daily_difference = temp_daily_difference\n",
    "        self.wind_daily_difference = wind_daily_difference\n",
    "        self.pressure_daily_difference = pressure_daily_difference\n",
    "        self.humidity_daily_difference = humidity_daily_difference\n",
    "        self.latitude_longnitude = latitude_longnitude\n",
    "        self.latitude_longitude_polynomial = 4\n",
    "        self.imputation = imputation\n",
    "        self.wind_transformation = wind_transformation\n",
    "        self.date_features = date_features\n",
    "    def fit(self, X, y):\n",
    "        pipeline_candidates = []\n",
    "        \n",
    "#         if self.latitude_longnitude:\n",
    "#             pipeline_candidates.append((\"latitude_longnitude\", MapLocation(X, normalize=False)))\n",
    "            \n",
    "#         if self.wind_transformation == \"wind_to_degrees\":\n",
    "#             pipeline_candidates.append((\"wind_to_degrees\",WindToDegrees()))\n",
    "#         elif self.wind_transformation == \"wind_to_binary\":\n",
    "#             raise Exception(\"not implemented\")\n",
    "#         else:\n",
    "#             pipeline_candidates.append((\"drop_wind\", DropColumns([\"WindGustDir\", \"WindDir9am\", \"WindDir3pm\"])))\n",
    "        #remove bad columns\n",
    "        #pipeline_candidates.append((\"Drop columns with NAs\", DropColumnsAbovePercentNA(0)))\n",
    "        \n",
    "        #normalization\n",
    "        pipeline_candidates.append((\"normalization\", NormalizeContinuousFeatures(MinMaxScaler())))\n",
    "        \n",
    "        if self.imputation == \"mean\":\n",
    "            #IMupte\n",
    "            pipeline_candidates.append((\"NA Mean Imputer\", MeanNANImputer()))\n",
    "                \n",
    "        #custom features\n",
    "        if self.temp_daily_difference:\n",
    "            pipeline_candidates.append((\"temp_daily_difference\", MaxMinTempDifference()))\n",
    "        if self.wind_daily_difference:\n",
    "            pipeline_candidates.append((\"wind_daily_difference\", WindDailyDifference()))\n",
    "        if self.pressure_daily_difference:\n",
    "            pipeline_candidates.append((\"pressure_daily_difference\", PressureDailyDifference()))\n",
    "        if self.humidity_daily_difference:\n",
    "            pipeline_candidates.append((\"humidity_daily_difference\", HumidityDailyDifference()))\n",
    "        if self.latitude_longnitude and self.latitude_longitude_polynomial:\n",
    "            pipeline_candidates.append((\"latitude_longnitude_polynomial\", PolynomialSubset(['longitude', 'latitude'], self.latitude_longitude_polynomial)))\n",
    "        \n",
    "#         if self.date_features:\n",
    "#             pipeline_candidates.append((\"Date\", FeaturesFromDate(True)))\n",
    "#         else:\n",
    "#             pipeline_candidates.append((\"Drop Date\", DropColumns([\"Date\"])))\n",
    "        \n",
    "        \n",
    "        pipeline_candidates.append((\"Drop Rest\", DropColumns(['Temp3pm','Temp9am','Humidity9am'])))\n",
    "        #add classifier\n",
    "        pipeline_candidates.append((\"classifier\", self.clf_class))\n",
    "        \n",
    "        self.pipeline = Pipeline(pipeline_candidates)\n",
    "        self.pipeline.fit(X,y)\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        return self.pipeline.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EachCityClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        self.classifiers = dict()# dict of pipelines\n",
    "    def fit(self, australia_rain, y):\n",
    "        main_pipeline_candidates = []\n",
    "        #drop without RainTommorrow\n",
    "        main_pipeline_candidates.append((\"drop_without_class\", DropColumnsWithNAs(\"RainTomorrow\")))\n",
    "        \n",
    "        australia_rain_by_city = {k:v for k, v in australia_rain.groupby('Location')}\n",
    "        \n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56420, 23)\n"
     ]
    }
   ],
   "source": [
    "australia_rain = pd.read_csv(FILE_PATH)\n",
    "australia_rain = australia_rain.dropna()\n",
    "print(australia_rain.shape)\n",
    "australia_rain = DropRowsWithNAInColums([\"RainTomorrow\"]).transform(australia_rain)\n",
    "australia_rain = DropRowsWithMoreThanXNA(10).transform(australia_rain)\n",
    "australia_rain = RainToNumerical().transform(australia_rain)\n",
    "# australia_rain = MeanNANImputer().fit_transform(australia_rain)\n",
    "# australia_rain = RemoveOutliers().fit_transform(australia_rain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mode\n",
    "for col in ['RainToday']:\n",
    "    australia_rain[col]=australia_rain[col].fillna(australia_rain[col].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['RainToday']:\n",
    "    australia_rain[col]=australia_rain[col].fillna(australia_rain[col].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "australia_rain = WindToDegrees().fit_transform(australia_rain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "australia_rain = MeanNANImputer(['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am','Humidity3pm','Pressure9am','Pressure3pm','Cloud9am','Cloud3pm','Temp9am','Temp3pm', 'WindDir9am', 'WindDir3pm', 'WindGustDir']).fit_transform(australia_rain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed: 1841\n"
     ]
    }
   ],
   "source": [
    "australia_rain = RemoveOutliers().fit_transform(australia_rain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "australia_rain = FeaturesFromDate(True).fit_transform(australia_rain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "australia_rain = MapLocation(australia_rain, normalize=False).fit_transform(australia_rain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "australia_rain = DropColumns(\"Location\").fit_transform(australia_rain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinTemp          0.0\n",
       "latitude         0.0\n",
       "longitude        0.0\n",
       "MaxTemp          0.0\n",
       "Rainfall         0.0\n",
       "Evaporation      0.0\n",
       "Sunshine         0.0\n",
       "WindGustDir      0.0\n",
       "WindGustSpeed    0.0\n",
       "WindDir9am       0.0\n",
       "WindDir3pm       0.0\n",
       "WindSpeed9am     0.0\n",
       "WindSpeed3pm     0.0\n",
       "Humidity9am      0.0\n",
       "Humidity3pm      0.0\n",
       "Pressure9am      0.0\n",
       "Pressure3pm      0.0\n",
       "Cloud9am         0.0\n",
       "Cloud3pm         0.0\n",
       "Temp9am          0.0\n",
       "Temp3pm          0.0\n",
       "RainToday        0.0\n",
       "RainTomorrow     0.0\n",
       "Week_Number      0.0\n",
       "Year             0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "australia_rain.isna().sum()/australia_rain.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86266, 24)\n"
     ]
    }
   ],
   "source": [
    "y = australia_rain[\"RainTomorrow\"].astype(int)\n",
    "X = australia_rain.drop(\"RainTomorrow\", axis =1)\n",
    "os = SMOTE()\n",
    "X, y = os.fit_resample(X,y)\n",
    "print(X.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = WholeRainClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "score=clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9126738794435858"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv = GridSearchCV(estimator=WholeRainClassifier(),\n",
    "             param_grid={'clf_class': [RandomForestClassifier,MLPClassifier]},\n",
    "            n_jobs=-1, cv=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RandomForestClassifier(n_jobs=-1), 12, True, False, False, False, True, 'mean', 'wind_to_degrees', True)\n",
      "0.9122150789012273\n",
      "(RandomForestClassifier(n_jobs=-1), 12, True, False, False, False, True, 'mean', None, True)\n",
      "0.9097926087634494\n",
      "(RandomForestClassifier(n_jobs=-1), 12, True, False, False, False, False, 'mean', 'wind_to_degrees', True)\n",
      "0.9089851475549696\n",
      "(RandomForestClassifier(n_jobs=-1), 12, True, False, False, False, False, 'mean', None, True)\n",
      "0.908266480641786\n",
      "(RandomForestClassifier(n_jobs=-1), 12, True, False, False, True, True, 'mean', 'wind_to_degrees', True)\n",
      "0.9107629958693789\n",
      "(RandomForestClassifier(n_jobs=-1), 12, True, False, False, True, True, 'mean', None, True)\n",
      "0.9118588492466235\n",
      "(RandomForestClassifier(n_jobs=-1), 12, True, False, False, True, False, 'mean', 'wind_to_degrees', True)\n",
      "0.9088234155406714\n",
      "(RandomForestClassifier(n_jobs=-1), 12, True, False, False, True, False, 'mean', None, True)\n",
      "0.9089781361451388\n",
      "(RandomForestClassifier(n_jobs=-1), 12, False, False, False, False, True, 'mean', 'wind_to_degrees', True)\n",
      "0.9134349300385859\n",
      "(RandomForestClassifier(n_jobs=-1), 12, False, False, False, False, True, 'mean', None, True)\n",
      "0.9122109122109121\n",
      "(RandomForestClassifier(n_jobs=-1), 12, False, False, False, False, False, 'mean', 'wind_to_degrees', True)\n",
      "0.9110317521931527\n",
      "(RandomForestClassifier(n_jobs=-1), 12, False, False, False, False, False, 'mean', None, True)\n",
      "0.9087591240875912\n",
      "(RandomForestClassifier(n_jobs=-1), 12, False, False, False, True, True, 'mean', 'wind_to_degrees', True)\n",
      "0.9104925053533192\n",
      "(RandomForestClassifier(n_jobs=-1), 12, False, False, False, True, True, 'mean', None, True)\n",
      "0.9112306133582729\n",
      "(RandomForestClassifier(n_jobs=-1), 12, False, False, False, True, False, 'mean', 'wind_to_degrees', True)\n",
      "0.9088795908879591\n",
      "(RandomForestClassifier(n_jobs=-1), 12, False, False, False, True, False, 'mean', None, True)\n",
      "0.9079360161121655\n",
      "(RandomForestClassifier(n_estimators=500, n_jobs=-1), 12, True, False, False, False, True, 'mean', 'wind_to_degrees', True)\n",
      "0.9127119962657539\n",
      "(RandomForestClassifier(n_estimators=500, n_jobs=-1), 12, True, False, False, False, True, 'mean', None, True)\n",
      "0.9129250641973388\n",
      "(RandomForestClassifier(n_estimators=500, n_jobs=-1), 12, True, False, False, False, False, 'mean', 'wind_to_degrees', True)\n",
      "0.9112210965895909\n",
      "(RandomForestClassifier(n_estimators=500, n_jobs=-1), 12, True, False, False, False, False, 'mean', None, True)\n",
      "0.9108581164807931\n",
      "(RandomForestClassifier(n_estimators=500, n_jobs=-1), 12, True, False, False, True, True, 'mean', 'wind_to_degrees', True)\n",
      "0.9134746257048415\n",
      "(RandomForestClassifier(n_estimators=500, n_jobs=-1), 12, True, False, False, True, True, 'mean', None, True)\n",
      "0.9116069346186738\n",
      "(RandomForestClassifier(n_estimators=500, n_jobs=-1), 12, True, False, False, True, False, 'mean', 'wind_to_degrees', True)\n",
      "0.9096462400927895\n",
      "(RandomForestClassifier(n_estimators=500, n_jobs=-1), 12, True, False, False, True, False, 'mean', None, True)\n",
      "0.9103496287128713\n",
      "(RandomForestClassifier(n_estimators=500, n_jobs=-1), 12, False, False, False, False, True, 'mean', 'wind_to_degrees', True)\n",
      "0.914421614877773\n",
      "(RandomForestClassifier(n_estimators=500, n_jobs=-1), 12, False, False, False, False, True, 'mean', None, True)\n",
      "0.9137206042672481\n",
      "(RandomForestClassifier(n_estimators=500, n_jobs=-1), 12, False, False, False, False, False, 'mean', 'wind_to_degrees', True)\n",
      "0.9121786669768525\n",
      "(RandomForestClassifier(n_estimators=500, n_jobs=-1), 12, False, False, False, False, False, 'mean', None, True)\n",
      "0.9115904426286644\n",
      "(RandomForestClassifier(n_estimators=500, n_jobs=-1), 12, False, False, False, True, True, 'mean', 'wind_to_degrees', True)\n",
      "0.9131567689495586\n",
      "(RandomForestClassifier(n_estimators=500, n_jobs=-1), 12, False, False, False, True, True, 'mean', None, True)\n",
      "0.9122547492992839\n",
      "(RandomForestClassifier(n_estimators=500, n_jobs=-1), 12, False, False, False, True, False, 'mean', 'wind_to_degrees', True)\n",
      "0.9104535270030133\n",
      "(RandomForestClassifier(n_estimators=500, n_jobs=-1), 12, False, False, False, True, False, 'mean', None, True)\n",
      "0.9103080018572978\n",
      "(XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
      "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "              learning_rate=None, max_delta_step=None, max_depth=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              random_state=None, reg_alpha=None, reg_lambda=None,\n",
      "              scale_pos_weight=None, subsample=None, tree_method=None,\n",
      "              validate_parameters=None, verbosity=None), 12, True, False, False, False, True, 'mean', 'wind_to_degrees', True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacper/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:44:36] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9127231964128434\n",
      "(XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=12, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None), 12, True, False, False, False, True, 'mean', None, True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacper/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:44:40] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9127231964128434\n",
      "(XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=12, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None), 12, True, False, False, False, False, 'mean', 'wind_to_degrees', True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacper/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:44:44] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9125871324413107\n",
      "(XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=12, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None), 12, True, False, False, False, False, 'mean', None, True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacper/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:44:48] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9125871324413107\n",
      "(XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=12, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None), 12, True, False, False, True, True, 'mean', 'wind_to_degrees', True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacper/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:44:52] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9125779127377337\n",
      "(XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=12, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None), 12, True, False, False, True, True, 'mean', None, True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacper/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:44:57] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9125779127377337\n",
      "(XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=12, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None), 12, True, False, False, True, False, 'mean', 'wind_to_degrees', True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacper/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:45:01] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9135318808714771\n",
      "(XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=12, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None), 12, True, False, False, True, False, 'mean', None, True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacper/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:45:05] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9135318808714771\n",
      "(XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=12, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None), 12, False, False, False, False, True, 'mean', 'wind_to_degrees', True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacper/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:45:09] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9140753191659663\n",
      "(XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=12, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None), 12, False, False, False, False, True, 'mean', None, True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacper/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:45:13] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9140753191659663\n",
      "(XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=12, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None), 12, False, False, False, False, False, 'mean', 'wind_to_degrees', True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacper/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:45:17] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9132906325060048\n",
      "(XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=12, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None), 12, False, False, False, False, False, 'mean', None, True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacper/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:45:20] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9132906325060048\n",
      "(XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=12, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None), 12, False, False, False, True, True, 'mean', 'wind_to_degrees', True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacper/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:45:23] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9133807886107334\n",
      "(XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=12, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None), 12, False, False, False, True, True, 'mean', None, True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacper/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:45:27] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9133807886107334\n",
      "(XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=12, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None), 12, False, False, False, True, False, 'mean', 'wind_to_degrees', True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacper/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:45:31] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9144523914172693\n",
      "(XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=12, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None), 12, False, False, False, True, False, 'mean', None, True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacper/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:45:35] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9144523914172693\n",
      "(DecisionTreeClassifier(), 12, True, False, False, False, True, 'mean', 'wind_to_degrees', True)\n",
      "0.8444564671329752\n",
      "(DecisionTreeClassifier(), 12, True, False, False, False, True, 'mean', None, True)\n",
      "0.8462967262595775\n",
      "(DecisionTreeClassifier(), 12, True, False, False, False, False, 'mean', 'wind_to_degrees', True)\n",
      "0.8452385566450913\n",
      "(DecisionTreeClassifier(), 12, True, False, False, False, False, 'mean', None, True)\n",
      "0.8464997286611365\n",
      "(DecisionTreeClassifier(), 12, True, False, False, True, True, 'mean', 'wind_to_degrees', True)\n",
      "0.8444153039185437\n",
      "(DecisionTreeClassifier(), 12, True, False, False, True, True, 'mean', None, True)\n",
      "0.8432982537296172\n",
      "(DecisionTreeClassifier(), 12, True, False, False, True, False, 'mean', 'wind_to_degrees', True)\n",
      "0.8424397426954972\n",
      "(DecisionTreeClassifier(), 12, True, False, False, True, False, 'mean', None, True)\n",
      "0.8444875024220113\n",
      "(DecisionTreeClassifier(), 12, False, False, False, False, True, 'mean', 'wind_to_degrees', True)\n",
      "0.8458773048822915\n",
      "(DecisionTreeClassifier(), 12, False, False, False, False, True, 'mean', None, True)\n",
      "0.8465173936623537\n",
      "(DecisionTreeClassifier(), 12, False, False, False, False, False, 'mean', 'wind_to_degrees', True)\n",
      "0.8449052500776639\n",
      "(DecisionTreeClassifier(), 12, False, False, False, False, False, 'mean', None, True)\n",
      "0.847315501378159\n",
      "(DecisionTreeClassifier(), 12, False, False, False, True, True, 'mean', 'wind_to_degrees', True)\n",
      "0.8462220850850465\n",
      "(DecisionTreeClassifier(), 12, False, False, False, True, True, 'mean', None, True)\n",
      "0.8431584640346133\n",
      "(DecisionTreeClassifier(), 12, False, False, False, True, False, 'mean', 'wind_to_degrees', True)\n",
      "0.8442562449011305\n",
      "(DecisionTreeClassifier(), 12, False, False, False, True, False, 'mean', None, True)\n",
      "0.8439413908054888\n",
      "(LogisticRegression(n_jobs=-1), 12, True, False, False, False, True, 'mean', 'wind_to_degrees', True)\n",
      "0.8124171281491304\n",
      "(LogisticRegression(n_jobs=-1), 12, True, False, False, False, True, 'mean', None, True)\n",
      "0.8124171281491304\n",
      "(LogisticRegression(n_jobs=-1), 12, True, False, False, False, False, 'mean', 'wind_to_degrees', True)\n",
      "0.8084823869405608\n",
      "(LogisticRegression(n_jobs=-1), 12, True, False, False, False, False, 'mean', None, True)\n",
      "0.8084823869405608\n",
      "(LogisticRegression(n_jobs=-1), 12, True, False, False, True, True, 'mean', 'wind_to_degrees', True)\n",
      "0.8124658709727748\n",
      "(LogisticRegression(n_jobs=-1), 12, True, False, False, True, True, 'mean', None, True)\n",
      "0.8124658709727748\n",
      "(LogisticRegression(n_jobs=-1), 12, True, False, False, True, False, 'mean', 'wind_to_degrees', True)\n",
      "0.8087448760491899\n",
      "(LogisticRegression(n_jobs=-1), 12, True, False, False, True, False, 'mean', None, True)\n",
      "0.8087448760491899\n",
      "(LogisticRegression(n_jobs=-1), 12, False, False, False, False, True, 'mean', 'wind_to_degrees', True)\n",
      "0.8124195498693295\n",
      "(LogisticRegression(n_jobs=-1), 12, False, False, False, False, True, 'mean', None, True)\n",
      "0.8124195498693295\n",
      "(LogisticRegression(n_jobs=-1), 12, False, False, False, False, False, 'mean', 'wind_to_degrees', True)\n",
      "0.8084823869405608\n",
      "(LogisticRegression(n_jobs=-1), 12, False, False, False, False, False, 'mean', None, True)\n",
      "0.8084823869405608\n",
      "(LogisticRegression(n_jobs=-1), 12, False, False, False, True, True, 'mean', 'wind_to_degrees', True)\n",
      "0.8124171281491304\n",
      "(LogisticRegression(n_jobs=-1), 12, False, False, False, True, True, 'mean', None, True)\n",
      "0.8124171281491304\n",
      "(LogisticRegression(n_jobs=-1), 12, False, False, False, True, False, 'mean', 'wind_to_degrees', True)\n",
      "0.8088378810945857\n",
      "(LogisticRegression(n_jobs=-1), 12, False, False, False, True, False, 'mean', None, True)\n",
      "0.8088378810945857\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "best_clf = 0\n",
    "clfs = [RandomForestClassifier(n_jobs=-1),RandomForestClassifier(n_jobs=-1, n_estimators=500), XGBClassifier(objective='binary:logistic'), DecisionTreeClassifier(), LogisticRegression(n_jobs=-1)]\n",
    "columns_na_threshold = [12]\n",
    "temp_daily_difference = [True, False]\n",
    "wind_daily_difference = [False]\n",
    "pressure_daily_difference = [False]\n",
    "humidity_daily_difference = [False, True]\n",
    "latitude_longnitude = [True, False]\n",
    "imputation = [\"mean\"]\n",
    "wind_transformation = [\"wind_to_degrees\", None]\n",
    "date_features = [True]\n",
    "combinations = itertools.product(clfs,\n",
    "                                 columns_na_threshold,\n",
    "                                 temp_daily_difference,\n",
    "                                 wind_daily_difference,\n",
    "                                 pressure_daily_difference,\n",
    "                                 humidity_daily_difference,\n",
    "                                 latitude_longnitude,\n",
    "                                 imputation,\n",
    "                                 wind_transformation,\n",
    "                                 date_features\n",
    "                                )\n",
    "for comb in list(combinations):\n",
    "    print(comb)\n",
    "    clf = WholeRainClassifier(*comb)\n",
    "\n",
    "    clf.fit(X_train,y_train)\n",
    "    predicted = clf.predict(X_test)\n",
    "    \n",
    "    score = f1_score(y_test, predicted)\n",
    "#     score = cross_val_score(clf, X, y, scoring=\"f1\", cv = 5, n_jobs=-1).mean()\n",
    "    print(score)\n",
    "    if score > best_score:\n",
    "        best_clf = clf\n",
    "        best_score = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9144523914172693"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf_class': XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "               importance_type='gain', interaction_constraints='',\n",
       "               learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "               n_estimators=100, n_jobs=12, num_parallel_tree=1, random_state=0,\n",
       "               reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "               tree_method='exact', validate_parameters=1, verbosity=None),\n",
       " 'columns_na_threshold': 12,\n",
       " 'date_features': True,\n",
       " 'humidity_daily_difference': True,\n",
       " 'imputation': 'mean',\n",
       " 'latitude_longnitude': False,\n",
       " 'pressure_daily_difference': False,\n",
       " 'temp_daily_difference': False,\n",
       " 'wind_daily_difference': False,\n",
       " 'wind_transformation': 'wind_to_degrees'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clf.get_params(deep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
